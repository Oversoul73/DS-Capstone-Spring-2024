# Multi-Model Emotion Detection System

### INTRODUCTION
In the period of human-computer interaction and full of feeling computing, understanding human feelings plays an urgent part in upgrading client involvement and engagement. To address this, our venture points to creating a modern multimodal feeling discovery framework leveraging facial expressions, voice sounds, and literary settings. By joining these different modalities, we look for a comprehensive arrangement capable of precisely recognizing and deciphering human feelings in real time. The system's backbone integrates the LLAMA (or LLAVA) model, a state-of-the-art deep learning architecture designed for multimodal learning tasks. Streamlit, a user-friendly web application framework, is used for the system's front end, providing an intuitive interface for users. MongoDB or Firebase are used for data storage and management, ensuring seamless integration with the emotion detection system. Flask is used for communication between the frontend and backend components, facilitating robust endpoints for data exchange. The project pushes the boundaries of emotion detection technology by leveraging synergies between facial, voice, and textual modalities. By integrating cutting-edge deep learning models, intuitive frontend interfaces, and robust backend infrastructure, the project aims to deliver a powerful multimodal emotion detection system capable of enriching human-computer interactions across various domains, including virtual assistants, social robotics, and affective computing applications.

### [Powerpoint slide] (https://docs.google.com/presentation/d/1UoOOKlQWId-QnC3Fcw4eUMsNSvg2EzN78zPmY966Xgk/edit#slide=id.g2b4849c857a_0_93)

### My Contributions:
In this Project, I have searched for some facial detection datasets and researched some of the models to implement like LLaVa.
Searched related projecctsd to see the scope of out project.
Contributed to the Project Initiation Documment and PowerPoint slides. 
