{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (4.38.1)\n",
      "Requirement already satisfied: datasets in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (2.17.1)\n",
      "Requirement already satisfied: torch in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: librosa in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: filelock in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from pooch>=1.0->librosa) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\coderepo\\ds-capstone-spring-2024\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch librosa scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming your data is in the form of a directory with subdirectories for each emotion\n",
    "data_dir = '../database/Emotions'\n",
    "emotions = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotions)\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    # Here you can add more preprocessing if needed, like padding or trimming silence\n",
    "    return audio\n",
    "\n",
    "# Create a dataset\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = preprocess_audio(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(audio, dtype=torch.float32), torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "# Collect all file paths and their labels\n",
    "file_paths = []\n",
    "labels = []\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(data_dir, emotion)\n",
    "    for filename in os.listdir(emotion_dir):\n",
    "        file_paths.append(os.path.join(emotion_dir, filename))\n",
    "        labels.append(emotion)\n",
    "\n",
    "# Encode labels to integers\n",
    "labels = label_encoder.transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(file_paths, labels, test_size=0.3, random_state=42)\n",
    "# Further split test set into validation and test sets\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(test_paths, test_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmotionDataset(train_paths, train_labels)\n",
    "val_dataset = EmotionDataset(val_paths, val_labels)\n",
    "test_dataset = EmotionDataset(test_paths, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8  # You may need to adjust this based on your GPU memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Load the processor and the model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    num_labels=len(emotions),\n",
    "    problem_type=\"single_label_classification\",\n",
    ")\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a custom collate function for padding\n",
    "def collate_fn(batch):\n",
    "    # Assume batch is a list of (sequence, label)\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the maximum length of any sequence\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert labels to a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return padded_sequences, labels\n",
    "\n",
    "# Create DataLoader with custom collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.16      0.05      0.07       305\n",
      "   Disgusted       0.00      0.00      0.00       290\n",
      "     Fearful       0.00      0.00      0.00       309\n",
      "       Happy       0.16      0.96      0.28       316\n",
      "     Neutral       0.00      0.00      0.00       276\n",
      "         Sad       0.00      0.00      0.00       324\n",
      "   Surprised       0.00      0.00      0.00       100\n",
      "\n",
      "    accuracy                           0.16      1920\n",
      "   macro avg       0.05      0.14      0.05      1920\n",
      "weighted avg       0.05      0.16      0.06      1920\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CodeRepo\\DS-Capstone-Spring-2024\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\CodeRepo\\DS-Capstone-Spring-2024\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\CodeRepo\\DS-Capstone-Spring-2024\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for audio_inputs, labels in test_loader:\n",
    "        audio_inputs = audio_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(audio_inputs)\n",
    "        \n",
    "        # Get the predictions\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Move predictions and labels to CPU and convert to numpy for sklearn\n",
    "        predicted_labels.extend(predictions.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert encoded labels back to original labels\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_labels)\n",
    "true_labels = label_encoder.inverse_transform(true_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=emotions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 170/1120 [30:33<2:50:44, 10.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss after epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_eval_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_values\u001b[38;5;241m=\u001b[39minput_values, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\CodeRepo\\DS-Capstone-Spring-2024\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\CodeRepo\\DS-Capstone-Spring-2024\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_values = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            outputs = model(input_values=input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Training loss after epoch {epoch + 1}: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in tqdm(val_loader):\n",
    "            with torch.no_grad():\n",
    "                input_values = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "                outputs = model(input_values=input_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "        print(f\"Validation loss after epoch {epoch + 1}: {total_eval_loss / len(val_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
